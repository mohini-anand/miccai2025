{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zz_MDZnASOqR"
      },
      "outputs": [],
      "source": [
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchaudio torchvision torchtext torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bnr8Z0-rGGR"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import math\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilB_SL_krLtG"
      },
      "outputs": [],
      "source": [
        "pkl_file_path = \"processed_fibers_longer_fibers.pkl\"\n",
        "with open(pkl_file_path, \"rb\") as f:\n",
        "    fibers = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hetaepirN-r",
        "outputId": "473e6f2e-52c0-47f7-ad92-b35bc761fb54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(322933, 80734, torch.Size([178, 5]), torch.Size([119, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_fibers, test_fibers = train_test_split(fibers, test_size=0.2, random_state=42)\n",
        "len(train_fibers), len(test_fibers), train_fibers[0].shape, test_fibers[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RLKRLnJsp0V",
        "outputId": "2c79ba5c-c5b3-48b7-c619-01ff93ee0b34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(290640, 32293, 80734)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "class FiberDataset(Dataset):\n",
        "    def __init__(self, fibers, predict_steps=25):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fibers (list of tensors): List of fibers, where each fiber is a tensor of shape (num_points, num_features).\n",
        "            predict_steps (int): Number of future steps to predict.\n",
        "        \"\"\"\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self.lengths = []\n",
        "        self.predict_steps = predict_steps\n",
        "\n",
        "        for fiber in fibers:\n",
        "            seq_len = len(fiber)\n",
        "            if seq_len > predict_steps:\n",
        "                self.inputs.append(fiber[: -(predict_steps)]) # all points except the last `predict_steps`\n",
        "                self.targets.append( torch.stack([fiber[i : i + predict_steps] for i in range(seq_len - predict_steps)]))\n",
        "                self.lengths.append(seq_len - predict_steps)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx], self.lengths[idx]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function to pad sequences and return lengths.\n",
        "\n",
        "    Args:\n",
        "        batch: List of tuples (inputs, targets, lengths).\n",
        "            inputs: Tensor of shape (seq_len, input_size).\n",
        "            targets: Tensor of shape (seq_len, predict_steps, input_size).\n",
        "            lengths: Sequence lengths.\n",
        "\n",
        "    Returns:\n",
        "        Padded inputs: Tensor of shape (batch_size, max_seq_len, input_size).\n",
        "        Padded targets: Tensor of shape (batch_size, max_seq_len, predict_steps, input_size).\n",
        "        Lengths tensor: Tensor of shape (batch_size,).\n",
        "    \"\"\"\n",
        "    inputs, targets, lengths = zip(*batch)\n",
        "\n",
        "    # Pad inputs to the same length\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True)  # Shape: (batch_size, max_seq_len, input_size)\n",
        "\n",
        "    # Pad targets to the same length\n",
        "    max_seq_len = max([t.size(0) for t in targets])  # Find the max sequence length in the batch\n",
        "    predict_steps = targets[0].size(1)  # Number of prediction steps (25 in your case)\n",
        "    input_size = targets[0].size(2)  # Number of features per point (5 in your case)\n",
        "\n",
        "    targets_padded = torch.zeros(len(targets), max_seq_len, predict_steps, input_size)\n",
        "    for i, target in enumerate(targets):\n",
        "        seq_len = target.size(0)\n",
        "        targets_padded[i, :seq_len, :, :] = target  # Copy the target data into the padded tensor\n",
        "\n",
        "    # Convert lengths to a tensor\n",
        "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return inputs_padded, targets_padded, lengths_tensor\n",
        "\n",
        "train_dataset = FiberDataset(train_fibers, predict_steps=25)\n",
        "test_dataset = FiberDataset(test_fibers, predict_steps=25)\n",
        "\n",
        "train_subset, val_subset = random_split(train_dataset, [0.9, 0.1])\n",
        "\n",
        "len(train_subset), len(val_subset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai8eWhRLswnh"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=12, collate_fn=collate_fn, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False, num_workers=12, collate_fn=collate_fn, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=12, collate_fn=collate_fn, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8wnkY_VqZmt"
      },
      "source": [
        "# Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzCdod9sqb5B"
      },
      "outputs": [],
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, predict_steps=25):\n",
        "      super(BidirectionalLSTM, self).__init__()\n",
        "      '''\n",
        "      input_size: the number of expected features in the input x\n",
        "      hidden_size: the number of features in the hidden state h\n",
        "      num_layers: number of recurrent layers.\n",
        "      '''\n",
        "      self.bilstm = nn.LSTM(\n",
        "          input_size=input_size,\n",
        "          hidden_size=hidden_size,\n",
        "          num_layers=num_layers,\n",
        "          bidirectional=True,\n",
        "          batch_first=True\n",
        "        )\n",
        "      self.predict_steps = predict_steps\n",
        "      self.fc = nn.Linear(hidden_size * 2, self.predict_steps * input_size)  # Output size matches input for next-point prediction\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "      # Pack the padded sequence\n",
        "      packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "      # Pass through BiLSTM\n",
        "      packed_out, (hidden, cell) = self.bilstm(packed_x)\n",
        "\n",
        "      # Unpack the sequence\n",
        "      out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "      # Fully connected layer for next-point prediction\n",
        "      output = self.fc(out)\n",
        "      batch_size, seq_len, feature_size = output.size()\n",
        "      assert feature_size == self.predict_steps * x.size(2), f\"Mismatch in feature size: expected {self.predict_steps * x.size(2)}, got {feature_size}\"\n",
        "\n",
        "      # Reshape to (batch_size, seq_len, predict_steps, input_size)\n",
        "      output = output.view(batch_size, seq_len, self.predict_steps, -1)\n",
        "\n",
        "      return output, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9X2wViUqppA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFUXqkm9pd4U"
      },
      "outputs": [],
      "source": [
        "model_name_2 = \"/content/drive/MyDrive/Fall 2024: ML Models/Model 1: Bi-directional LSTM/bilstm_fiber_25_points_v2.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUC2gM0Vth0z"
      },
      "outputs": [],
      "source": [
        "def load_blstm_model(load_path, device, hidden_size, num_layers):\n",
        "  model = BidirectionalLSTM(input_size=5, hidden_size=hidden_size, num_layers=num_layers)\n",
        "  model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "  model.to(device)\n",
        "  print(\"Model loaded successfully!\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI1hplwnvKr5"
      },
      "outputs": [],
      "source": [
        "model_2 = load_blstm_model(model_name_2, device, hidden_size=256, num_layers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkShHuIJtlqh"
      },
      "outputs": [],
      "source": [
        "def test_blstm_model(model, dataloader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets, lengths in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "        inputs = inputs[perm_idx]\n",
        "        targets = targets[perm_idx]\n",
        "\n",
        "        outputs, _ = model(inputs, lengths)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(f\"Test Loss for batch: {loss.item()}\")\n",
        "\n",
        "  return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs errors for x, y, z, HA, TA separately\n",
        "def test_blstm_model_per_feature(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_loss_per_feature = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, lengths in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Sort the sequences by length for the BLSTM\n",
        "            lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "            inputs = inputs[perm_idx]\n",
        "            targets = targets[perm_idx]\n",
        "\n",
        "            outputs, _ = model(inputs, lengths)\n",
        "\n",
        "            # Compute squared error without reduction:\n",
        "            squared_error = (outputs - targets) ** 2  # Shape: (B, L, input_dim)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            # If your data has variable lengths and padded elements, construct a mask:\n",
        "            max_seq_len = outputs.shape[1]\n",
        "            mask = torch.arange(max_seq_len, device=device)[None, :] < lengths[:, None]  # Shape: (B, L)\n",
        "            mask = mask.unsqueeze(-1).unsqueeze(-1) # Add two more dimensions to match the target tensor\n",
        "            mask = mask.expand(-1, -1, outputs.shape[2], outputs.shape[3]) # Expand to match the size of squared_error\n",
        "\n",
        "            # Zero out padded positions:\n",
        "            squared_error = squared_error * mask\n",
        "\n",
        "            # Compute the mean error per feature over valid points:\n",
        "            valid_counts = mask.sum(dim=(0, 1))  # Shape: (1, input_dim)\n",
        "            loss_per_feature = squared_error.sum(dim=(0, 1)) / valid_counts\n",
        "            all_loss_per_feature.append(loss_per_feature)\n",
        "\n",
        "    # Aggregate the losses over all batches:\n",
        "    aggregated_loss = torch.stack(all_loss_per_feature, dim=0).mean(dim=0)\n",
        "    return aggregated_loss"
      ],
      "metadata": {
        "id": "LpfrPUY-ERt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ6_EfWUvXTP"
      },
      "outputs": [],
      "source": [
        "#test_blstm_model(model_1, test_dataloader, device)\n",
        "model_1_rseults = test_blstm_model_per_feature(model_1, test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_rseults.mean(dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJVdBjJnMF2l",
        "outputId": "34336510-ebb5-4568-c31d-1a75ae7062fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0182e-01, 1.0780e-01, 6.2997e-02, 2.4478e+00, 1.9589e-03],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DKmwixIuYhP"
      },
      "outputs": [],
      "source": [
        "#test_blstm_model(model_2, test_dataloader, device)\n",
        "model_2_rseults = test_blstm_model_per_feature(model_2, test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtEvkq21mevx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "def visualize_blstm_fiber(inputs, targets, outputs, lengths, fiber_idx):\n",
        "    \"\"\"\n",
        "    Visualize the input fiber, target points, and predicted points for one fiber processed by BLSTM.\n",
        "\n",
        "    Args:\n",
        "        inputs (Tensor): Input tensor (batch_size, seq_len, input_size).\n",
        "        targets (Tensor): Target tensor (batch_size, seq_len, predict_steps, input_size).\n",
        "        outputs (Tensor): Model outputs (batch_size, seq_len, predict_steps, input_size).\n",
        "        lengths (Tensor): Lengths of valid sequences in the batch.\n",
        "        fiber_idx (int): Index of the fiber in the batch to visualize.\n",
        "    \"\"\"\n",
        "    # Unpack the fiber corresponding to the given fiber_idx\n",
        "    seq_len = lengths[fiber_idx].item()\n",
        "    input_fiber = inputs[fiber_idx, :seq_len, :3].cpu().numpy()  # [x, y, z] of input points\n",
        "    target_points = targets[fiber_idx, :seq_len, :, :3].cpu().numpy()  # [x, y, z] of target points\n",
        "    predicted_points = outputs[fiber_idx, :seq_len, :, :3].detach().cpu().numpy()  # [x, y, z] of predicted points\n",
        "\n",
        "    # Last valid input point\n",
        "    last_input_point = input_fiber[-1]  # Last point of the input trajectory\n",
        "\n",
        "    # Target and predicted next 25 points from the last input point\n",
        "    target_next_points = target_points[-1]  # Shape: (predict_steps, 3)\n",
        "    predicted_next_points = predicted_points[-1]  # Shape: (predict_steps, 3)\n",
        "\n",
        "    # Plot the input fiber trajectory and next points\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot the full input trajectory\n",
        "    ax.plot(input_fiber[:, 0], input_fiber[:, 1], input_fiber[:, 2], label=\"Input Fiber Trajectory\", color='blue')\n",
        "\n",
        "    # Plot the target next points\n",
        "    ax.scatter(target_next_points[:, 0], target_next_points[:, 1], target_next_points[:, 2], label=\"Target Next Points\", color='green')\n",
        "\n",
        "    # Plot the predicted next points\n",
        "    ax.scatter(predicted_next_points[:, 0], predicted_next_points[:, 1], predicted_next_points[:, 2], label=\"Predicted Next Points\", color='red')\n",
        "\n",
        "    # Formatting the plot\n",
        "    ax.set_title(\"BLSTM Fiber Visualization: Trajectory and Predictions\")\n",
        "    ax.set_xlabel(\"X-axis\")\n",
        "    ax.set_ylabel(\"Y-axis\")\n",
        "    ax.set_zlabel(\"Z-axis\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "inputs, targets, lengths = next(iter(test_dataloader))\n",
        "outputs_1, _ = model_1(inputs.to(device), lengths)\n",
        "outputs_2, _ = model_2(inputs.to(device), lengths)\n",
        "visualize_blstm_fiber(inputs, targets, outputs_1, lengths, fiber_idx=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGnKYy9zuo6L"
      },
      "outputs": [],
      "source": [
        "visualize_blstm_fiber(inputs, targets, outputs_2, lengths, fiber_idx=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ppy50oCJmwB5"
      },
      "outputs": [],
      "source": [
        "for fiber_idx in range(120):\n",
        "  visualize_blstm_fiber(inputs, targets, outputs_1, lengths, fiber_idx=fiber_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0YYW196tjunf"
      },
      "outputs": [],
      "source": [
        "for fiber_idx in range(120):\n",
        "  visualize_blstm_fiber(inputs, targets, outputs_2, lengths, fiber_idx=fiber_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_v3xBjtu2wW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}